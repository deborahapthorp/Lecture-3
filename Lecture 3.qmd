---
title: "PSYC 422 Lecture 3"
author: "A/Prof Deborah Apthorp"
format: 
  revealjs: 
    theme: sky
    incremental: true
    preview-links: auto
    slide-number: true
    logo: images/UNE_logo.png
    css: logo.css
editor: source
---

# Power, bias and research ethics - how to be a better researcher (Part II)

## Outline

- Cautionary Tales from Psychology research
- Questionable Research Practices
- Possible solutions
- Replicability and replication projects
- How open should we be? 
- Preregistration and Registered Reports
- What you can do! 

# Advice of a famous social psychologist 
“There are two possible articles you can write: (1) the article you planned to write when you designed your study or (2) the article that makes the most sense now that you have seen the results. They are rarely the same, and the correct answer is (2).” 

-Bem, 2003, pp. 171-172

##  A cautionary tale I: Daryl Bem

::: columns
:::{.column width="70%"}

- Daryl Bem, 2011, Journal of Personality & Social Psychology (not open access - cited 631 times)
- Respected researcher, top-tier journal
- Published 9 experiments which claimed to show effects of precognition
- Took established psychological experiments & time-reversed them

:::
:::{.column width="30%"}
::: fragment
![](images/bem.png)
:::
:::
:::


::: notes
Bem used a variety of techniques but the general approach was to "time reverse" established psychological effects. For example, the experiment that produced the largest effect size (Experiment 9) took as its starting point the trivial observation that memory for words is better if one is allowed to rehearse the words as opposed to being exposed to them just once. Of course, this usually involves rehearsing the words before one's memory for them is tested.

The astonishing claim made by Bem – apparently supported by his experimental data – was that memory for words is improved even if the rehearsal does not take place until after recall has been tested. The effect was dubbed the "retroactive facilitation of recall".
:::

##  A cautionary tale I: Daryl Bem

- Encouraged others to replicate - even provided software
- Researchers from 3 institutions (Stuart Ritchie, Chris French, and Richard Wiseman) [decided to do this](https://www.theguardian.com/science/2012/mar/15/precognition-studies-curse-failed-replications)
- Pre-registered 
- Failed to replicate any of the effects


##  A cautionary tale I: Daryl Bem

- Submitted to original journal (JPSP)
- Rejected without review - “we do not publish replications”
- Also rejected without review by Science Brevia & Psychological Science
- Reviewed at British Journal of Psychology - rejected (Bem was a reviewer)
- Finally published in PLoS One ([“Failing the Future”](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0033423)) (open access - cited 117 times)

## Side note - citations?
### …do they support, mention or contradict?

- New platform [scite.ai](https://scite.ai/) uses machine learning to determine what KIND of citations a paper gets
- Supporting, mentioning, contradicting 
- Most studies “mention” Bem rather than supporting 
- Of the 5 “supporting” citations, 3 are by Bem himself and the other 2 don’t actually support it

## A cautionary tale II: Diderick Stapel

- Diderick Stapel - famous Dutch social psychologist
- Received the "Career Trajectory Award" from the Society of Experimental Social Psychology in 2009
- 130 articles, 24 book chapters
- Key findings: Messy environments promote stereotyping; meat eaters “more selfish” than vegetarians
- Students never saw raw data; results “too good to be true”?

## A cautionary tale II: Diderick Stapel

::: columns
:::{.column width="70%"}
- Eventually, students blew the whistle
- All the data had been fabricated 
- At least 50 articles retracted
- How did he go unchallenged for so long? 
- Why did he do it? 
- Free translation of his book [here](http://nick.brown.free.fr/stapel)! 
:::

:::{.column width="30%"}
::: fragment
![](images/stapel.png)
:::
:::
:::
::: footer 
[http://retractionwatch.com/category/diederik-stapel/](http://retractionwatch.com/category/diederik-stapel/)

:::
# Quote 

"Outright fraud is somewhat impossible to estimate, because if you're really good at it you wouldn't be detectable," said Simonsohn, a social psychologist. "It's like asking how much of our money is fake money – we only catch the really bad fakers, **the good fakers we never catch**."

::: footer
[http://www.theguardian.com/science/2012/sep/13/scientific-research-fraud-bad-practice](http://www.theguardian.com/science/2012/sep/13/scientific-research-fraud-bad-practice)
:::

## A cautionary tale III - John Bargh

-  Classic finding: “social priming” - participants walked out of lab more slowly after being primed with “elderly”-related words (Bargh, Chen & Burrows, 1996)
- Cited over 2000 times - but not replicated often
- Stephane Doyen & colleagues attempted replication
- A few changes - more Ss, double blinding, and infrared sensors
- Effect did not replicate

::: footer

[http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029081](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029081)

:::

::: notes
This time, the priming words had no impact on the volunteers’ walking speed. They left the test room neither more slowly nor more quickly than when they arrived. Doyen suspected that Bargh’s research team could have unwittingly told their volunteers how they were meant to behave… Perhaps they themselves moved more slowly if they expected the volunteer to do so. Maybe they spoke more languidly, or shook hands more leisurely… Maybe they were responsible for creating the very behaviour they expected to see.
To test that idea, Doyen repeated his experiment with 50 fresh volunteers and 10 fresh experimenters. The experimenters always stuck to the same script, but they knew whether each volunteer had been primed or not. Doyen told half of them that people would walk more slowly thanks to the power of priming, but he told the other half to expect faster walks.

…He found that the volunteers moved more slowly only when they were tested by experimenters who expected them to move slowly… Let that sink in: the only way Doyen could repeat Bargh’s results was to deliberately tell the experimenters to expect those results.
:::

## Barghed! 
::: fragment
![](images/barghed.png)
:::

- Ed Yong’s (2012) piece about this is [here](https://www.nationalgeographic.com/science/article/failed-replication-bargh-psychology-study-doyen) (OSF version [here](https://osf.io/esgdh)).

::: notes
As stated before, Bargh also directs personal attacks at the authors of the paper (“incompetent or ill-informed”), at PLoS (“does not receive the usual high scientific journal standards of peer-review scrutiny”), and at me [Ed Yong] (“superficial online science journalism”). The entire post is entitled “Nothing in their heads”.

:::

## Questionable research practices (QRPs)

::: columns
:::{.column width="60%"}
![](images/qrps.png){.fragment width="540"}
:::

:::{.column width="40%"}
- Fig. 1. The nine circles of scientific hell (with apologies to Dante and xkcd). &copy; Association for Psychological Science.
:::
:::

::: footer
[Article by Neuroskeptic](https://journals.sagepub.com/doi/10.1177/1745691612459519)
:::

## Neuroskeptic, 2012 

::: columns
:::{.column width="50%"}
- First Circle: Limbo
- Second Circle: Overselling
- Third Circle: Post-Hoc Storytelling
- Fourth Circle: p Value Fishing
- Fifth Circle: Creative Use of Outliers
:::

:::{.column width="50%"}
- Sixth Circle: Plagiarism
- Seventh Circle: Non-publication of Data
- Eighth Circle: Partial Publication of Data
- Ninth Circle: Inventing Data
:::
:::

::: notes
Limbo? “Those who have committed no scientific sins per se, but who have turned a blind eye to them” 
Overselling - clickbait/hype/abstracts that don’t represent the results 

Post-hoc storytelling = HARKing 

P-value fishing otherwise known as p-hacking

Creative use of outliers - also p-hacking (related to optional stopping??)

Plagiarism - obviously everyone knows from undergrad, but what about self-plagiarism (some v high profile cases recently)

Non-publication = file drawer 

Partial publication - way too common. Have been asked to do it by reviewers myself. 

Inventing data - see Stapel. Also might be more common than we think (if people are good at it we won’t catch them). 
:::

## How common are QRPs? 

::: fragment
![](images/qrp_article.png)
:::
::: footer

Podcast about this paper [here!](https://soundcloud.com/reproducibilitea/episode-3-questionable-research-practices)

:::

# Prevalence of QRPs in Psychology

## Bayesian truth serum??

- Half subjects told that the truthfulness of their responses would be determined by an algorithm (BTS)
- This would determine the value of a donation to the charity of their choice
- This was true!
- The use of BTS increased % who admitted to QRPs, particularly those they judged less defensible
- Overall percentage very high!! 

## {}

![](images/qrp_prevalence.png){.fragment width=800 fig-align="center"}

## Why are we here? 

- As scientists, we want to know what is true.
- **Systematic replications** are a tool of scientific progress:
  - How reliable is the data published in journals?
  - What practices would lead to a higher level of replicability?

## The Reproducibility Project (RP)

::: columns 
:::{.column width="60%"}
- Aim: How reproducible are results in Psychology? 
- 255 volunteer researchers
- 64 universities 
- 11 countries
- Attempt to replicate 97 key findings in Psychology
:::
:::{.column width="40%"}
![](images/cos.png){.fragment}
:::
:::

## Estimating the reproducibility of psychological science

::: columns 
:::{.column width="50%"}

![](images/estimating.png){.fragment}

:::
:::{.column width="50%"}
- Now published in “Science”
- Only about half of studies replicated
- Generated lots of comment! 
- Check it out at [Science](science.sciencemag.org/content/349/6251/aac4716)
- Podcast about it [here](https://www.listennotes.com/podcasts/reproducibilitea/episode-4-reproducibility-now-G2qAagn4sFh/). 
:::


:::

::: footer 
[OPEN SCIENCE COLLABORATION. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.](https://doi.org/10.1126/science.aac4716)

:::

## Basic findings 

::: fragment
![](images/rp_pvalues_effects.png)
:::

- Approximately $2/3^{rds}$ of studies did not replicate using a p-value cutoff
- ([Cancer Biology](https://elifesciences.org/articles/71601): 54%)

::: notes

Overall, 36% of the replications yielded significant findings (p value below .05) compared to 97% of the original studies that had significant effects
 The mean effect size in in the replications was approximately half the magnitude of the effects reported in the original studies.
Studies in the field of cognitive psychology had a higher replication rate (50%) than studies in in the field of social psychology (25%).
:::

## Basic findings

::: columns 
:::{.column width="50%"}
![](images/pvalues_orig.png){.fragment}
:::

:::{.column width="50%"}
- No original study with a $p > . 025$ replicated
- When combined in meta-analysis with original studies, $1/3^{rd}$ no longer had sufficient evidence for existing
:::

:::

## Effect sizes and p-values

::: columns 
:::{.column width="70%"}
![](images/orig_vs_rep.png){.fragment}
:::

:::{.column width="30%"}

- Almost all replications had a smaller effect size than originals
- Diagonal line = equal effect size
:::

:::

::: notes

Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. 
Overall, original study effect sizes (M = 0.403, SD = 0.188) were reliably larger than replication effect sizes (M = 0.197, SD = 0.257), Wilcoxon’s W = 7137, P < 0.001. 
Comparing effect sizes across indicators, surprisingness of the original effect, and the challenge of conducting the replication were related to replication success for some indicators. Surprising effects were less reproducible, as were effects for which it was more challenging to conduct the replication.

:::

## Explanations for replication failures

::: columns 
:::{.column width="50%"}
- **Original studies:**
- Emphasis on innovation
- Emphasis on positive results
- Low power
- Flexibility in data analysis/collection
- Focus on clean and tidy results
:::

:::{.column width="50%"}
- **Replication studies:**
- False negatives
- Sampling error
- Low fidelity
:::

:::

## Investigating variability in replicability: Many Labs project

- One possible way of making psychology studies more robust is to distribute them across multiple labs
- This was the genesis of the first Many Labs project - now known as [Many Labs 1](https://psycnet.apa.org/fulltext/2014-20922-002.html) ([OSF site here](https://osf.io/wx7ck/))
- Also offered the opportunity to look at variations across sample and setting 
- Now followed up by Many Labs [2](https://journals.sagepub.com/doi/10.1177/2515245918810225), [3](https://www.sciencedirect.com/science/article/abs/pii/S0022103115300123?via%3Dihub) and [4](https://online.ucpress.edu/collabra/article/8/1/35271/168050/Many-Labs-4-Failure-to-Replicate-Mortality) ([5](https://journals.sagepub.com/doi/full/10.1177/2515245920958687), [Many Babies](https://manybabies.github.io/) 1--6, [#EEGMany Labs](https://www.sciencedirect.com/science/article/pii/S0010945221001106?via%3Dihub), and [Many Dogs](https://manydogsproject.github.io/) (those are the ones I know of!)

## Many Labs 1 - findings {.smaller}

::: columns 
:::{.column width="70%"}
![](images/manylabs1.png){.fragment}
:::

:::{.column width="30%"}
- 13 classic findings; 36 independent samples; 6,344 participants. 
- Preregistered protocol
- 10 effects replicated, 1 borderline, 2 didn’t
- Not much difference between US and international studies
:::
:::

## Many Labs 2 {.smaller}
### Investigating replicability across sample and setting

- [Many Labs 2](https://cos.io/about/news/28-classic-and-contemporary-psychology-findings-replicated-more-60-laboratories-each-across-three-dozen-nations-and-territories/) - now published in [Advances in Methods & Practices in Psychological Science](https://journals.sagepub.com/doi/10.1177/2515245918810225) (open access)
- 186 researchers, 28 classic & contemporary findings, 60 laboratories
- MEDIAN sample size 7,157! 
- **14/28** failed to replicate (by strict criterion of p <.0001) in spite of very large sample size
- Effect size less than half of original, on average
- Little evidence that replication depended on sample


## {background-color="white"}

![](images/manylabs2.png){.absolute top=0 right=180 width="750"}

## Prediction markets

- A second related paper had people bet on which papers would replicate
- Now published in [Journal of Economic Psychology](https://www.sciencedirect.com/science/article/pii/S0167487018303283)
- Markets correctly predicted 75% of the replication outcomes
- Survey & markets also predicted effect sizes
- Can you predict which studies will replicate? 
- [Play the online game and find out!](https://80000hours.org/psychology-replication-quiz/)

::: notes

 A second paper “Predicting Replication Outcomes in the Many Labs 2 Study” has been published in the Journal of Economic Psychology.  This paper reported evidence that researchers participating in surveys and prediction markets about the Many Labs 2 findings could predict which of the studies were likely to replicate and which were not.  In prediction markets, each share for a finding that successfully replicates is worth $1 and each share for a finding that fails to replicate is worth nothing. Researchers then buy and sell shares in each finding to predict which ones will succeed and fail to replicate.  The final market price is interpretable as the predicted probability that the original finding will replicate. Anna Dreber, senior author of the prediction market paper, and Professor at the Stockholm School of Economics and University of Innsbruck said “We now have four studies successfully demonstrating that researchers can predict whether findings will replicate or not in surveys and prediction markets with pretty good accuracy. This suggests potential reforms in peer review of grants and papers to help identify findings that are exciting but highly uncertain to invest resources to see if they are replicable.”
 
:::

## Other Many Labs projects

- [Many Labs 3](https://www.sciencedirect.com/science/article/pii/S0022103115300123) - Evaluating participant pool quality across the academic semester via replication
- [Many Labs 4](https://online.ucpress.edu/collabra/article/8/1/35271/168050/Many-Labs-4-Failure-to-Replicate-Mortality) - Failure to replicate Mortality Salience effect
- [Many Labs 5](https://journals.sagepub.com/doi/full/10.1177/2515245920958687) - Testing pre-data-collection peer review as an intervention to increase replicability
- [Many Babies](https://manybabies.github.io/) - [Many Babies 1](https://journals.sagepub.com/doi/10.1177/2515245919900809) now published - projects up to no. 4!
- [A thoughtful critique](https://www.cos.io/blog/critique-many-labs-projects) of the Many Labs projects by Charlie Ebersole (one of the project leaders). 

::: notes

In Many Labs 5, this was testing the claim in the Reproducibility project that the failure to replicate was due to researchers not adhering to the original protocols. Spoiler alert - it wasn’t. 

:::

## A similar approach - the Psych Science Accelerator

- A new initiative, aimed at providing psychological science with much greater power
- Website [here](https://psysciacc.org/) - introductory paper [here](https://journals.sagepub.com/doi/10.1177/2515245918797607)
- Currently 1328 labs representing 84 countries on all 6 populated continents
- New studies and replications; democratically selected
- Podcast [here](https://everythinghertz.com/78) (note - I get a mention at the start!)

## Other initiatives to solve the "reproducibility crisis"

::: columns 

:::{.column}
- Open access publishing
- Open data 
- Open code
- Preprints
- Preregistration
- Registered Reports
:::

:::{.column}
- Double blind peer review
- Open peer review 
  - Publish peer reviews
- Pay peer reviewers? 
- Post-publication peer review

:::
:::


## Open access publishing 
### Issues

- Early open access journals (e.g. PLoS One) were established to break the selectivity of traditional journals for "novelty" and publishing only significant results
- Currently, most (but not all!) open access journals require the author to pay for publication
- Costs vary (PeerJ - \$399 - 499 for life; PLoS One - US\$1350 per article; Nature - US\$11,390!)
- Costs can be claimed on grants - but not everyone has a grant
- Proliferation of “predatory” open access journals

## Open data

- In some fields, this has been the norm for decades (astronomy, climate science)
- In psychology, still very uncommon (even less so in neuroscience)
- Recently, some journals have introduced policies that authors must share their data
- PLoS - introduced policy in 2014, but not always adhered to
- Psych Science - Badges (but are they any use? Aha! Funny that you ask...)

## The Data Badge Project

## Preprints

## Preregistration

## Registered reports

## Reforming peer review
### Double-blind peer review

## Reforming peer review
### Open peer review

## Reforming peer review
### Paying peer reviewers

## Reforming peer review
### Post-publication peer review


## What you can do 

## Collaborative replication for undergraduates (CREP)

## FORRT


